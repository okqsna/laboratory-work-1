

### Алгоритм Крускала  
Алгоритм Крускала знаходить мінімальне остовне дерево для зв’язного графа з вагами на ребрах.  

#### Деталі реалізації  
На початку функція отримує випадково згенерований граф.  

Далі ми витягуємо ребра, сортуємо їх за вагою (це необхідно для роботи алгоритму) та ініціалізуємо два словники `mstk_v` і `mstk` — вони містять інформацію про вершини та ребра мінімального остовного дерева і оновлюються під час виконання алгоритму.  

#### Додаткова функція для об'єднання груп  
Під час виконання алгоритму Крускала ми групуємо вершини та ребра відповідно до їхніх зв’язків і ваг цих зв’язків. Але якщо ми знаходимо ребро, яке з'єднує дві вершини з різних груп, потрібно об'єднати ці групи в одну (фактично, ми просто додаємо меншу групу до більшої).  
Властивість `key_1` отримує ключ групи у словнику, до якої потрібно додати групу, позначену `key_2` у цьому ж словнику (це стосується як словника вершин, так і словника ребер).  

#### Основний алгоритм  
Після сортування всіх ребер у порядку зростання ми ітеруємося по них.  
На кожній ітерації розглядаємо ребро `(v, u)`.  
Перебираємо словник згрупованих вершин, кожен елемент якого має вигляд `(номер_групи, [список_вершин])`, і перевіряємо, чи вершина `v` належить якійсь групі. Якщо так — зберігаємо ключ групи у `key_0`. Те саме робимо для вершини `u`, зберігаючи результат у `key_1`.  

#### Обробка умов  
1. Якщо обидві вершини ще не належать жодній групі, створюємо для них нову групу. Аналогічно додаємо ребро у словник `mstk`.  
2. Якщо обидві вершини вже знаходяться в одній групі, додавання цього ребра створить цикл, тому ми його пропускаємо.  
3. Якщо одна вершина належить групі, а інша ще ні (є "сиротою"), ми додаємо сирітську вершину до групи та відповідно додаємо ребро.  
4. Якщо вершини знаходяться у двох різних групах — викликаємо функцію об'єднання груп.  

Ось і все!  

---

### Алгоритм Беллмана-Форда  

#### Деталі реалізації  
На початку функція отримує випадково згенерований граф, стартову вершину для виконання алгоритму та параметр для відображення відстаней у текстовому форматі.  

Далі ми витягуємо список ребер, вершин, стартову вершину та ініціалізуємо `distance_dict` — словник, що містить інформацію про відстані (ключі — вершини, значення — відповідні відстані).  

#### Запуск алгоритму  
Спочатку всі відстані встановлюємо в нескінченність, окрім стартової вершини, для якої відстань дорівнює 0.  

Основна частина алгоритму:  
- Ми повторюємо ітерацію `v-1` разів, де `v` — кількість вершин у графі.  
- На кожній ітерації ми перевіряємо для кожного ребра `(u, v)`, чи сума поточної відстані до `u` та ваги ребра є меншою за поточну відстань до `v`. Якщо так — оновлюємо відстань. Якщо вершина `u` має відстань "нескінченність", ми тимчасово замінюємо її на `0` для обчислення.  

Після основних ітерацій виконується ще одна додаткова перевірка на наявність від’ємних циклів. Якщо після цієї ітерації значення відстаней змінилися, це означає, що у графі є від’ємний цикл.  

Як результат, виводимо відстані у вигляді текстового опису та словника.  

---

### Класифікатор на основі дерева рішень  

#### Використовуваний набір даних  
Для побудови дерева рішень використовується набір даних про рак молочної залози. Він містить інформацію про те, чи є пухлина злоякісною або доброякісною, а також 8 інших характеристик, які допомагають у дослідженні.  

#### Опис алгоритму  
1. Спочатку ініціалізується клас для вузла дерева рішень і функція, яка перевіряє, чи є вузол листком.  
2. Далі ініціалізується основний клас дерева рішень, який керує всіма процесами алгоритму.  
3. Функція `fit` керує всіма етапами навчання дерева.  
4. `build_tree` — це рекурсивна функція, що будує дерево на основі таких критеріїв:
   - Пошук найкращої ознаки та порогового значення для поділу вузла на ліву та праву піддерева.  
   - Рекурсія триває до досягнення максимальної глибини, поки кількість зразків у вузлі не стане меншою за певний поріг або поки всі мітки не стануть однаковими.  

У `build_tree` використовуються допоміжні функції:  
- `find_best_split` — знаходить найкращу ознаку та поріг для поділу, який забезпечує максимальний приріст інформації.  
- `calculate_info_gain` — обчислює приріст інформації, використовуючи індекс Джині.  
- `gini` — функція для обчислення індексу Джині за стандартною формулою з використанням бібліотеки `numpy`.  
- `split` — ділить дані на ліве та праве піддерево відповідно до порогового значення.  

Функції `traverse_tree` та `predict` використовуються для збереження інформації про передбачення.  

Остаточна функція `evaluate` оцінює точність моделі на тестових даних із `scikit-learn`.  
